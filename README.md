# project-1-laurenhvomberg
project-1-laurenhvomberg created by GitHub Classroom
Participants will be asked to judge how easy it is to hear one or two melodies when interleaved melodies are presented. The interleaved melodies will vary in tonal relatedness, interonset interval, and pitch spread. We are interested in the role tonality (specifically) plays in the streaming of auditory stimuli while controlling for pitch spread and interonset interval. 
Participants will be presented with a consent form and questionnaire assessing their musical background (included as PDFs) before they begin the experiment. Once consent is obtained, they will be presented with instructions (via Psychopy). They will then be asked to indicate their understanding of the instructions by proceeding to the main experiment (pressing the “spacebar”). The first melody will then play, followed by a rating scale. Participants will be asked to identify how many melodies they heard and how easy it was to hear those melodies by clicking on the appropriate “tick” (“easy to hear one melody,” “moderately able to hear one melody,” “hard to hear one melody,” “hard to hear two melodies,” “moderately able to hear two melodies,” and “easy to hear two melodies”). They will then need to hit “enter/return” in order to proceed to the next melody. The judgments will be recorded in an excel spreadsheet. Responses (“ticks”) will be recorded (columns labelled “rating.response”) as well as the reaction times (“rating.rt”).
Melodies varied along three different tonalities; tonally the same (C major and C major), tonally related (C major and G major) and tonally unrelated (C major and F# major). 8-note melodies were written for each tonality in a tonal pair and then interleaved together, resulting in 16-note melodies for each tonality. The 8-note melodies were run through the Krumhansl-Schmuckler key-finding algorithm in order to ensure participants would perceive the intended tonality. This algorithm identifies correlations between pitch classes and their associated tonality (within pre-defined major and minor keys according to Western musical notation) in order to determine the strength of perceived tonality. The 8-note melody with the highest correlation (based on the Krumhansl-Schmuckler key-finding algorithm) was included in this project. For the purpose of this project, two 16-note melodies (per tonal pair) were included for each IOI and pitch spread in order to control for starting note (especially when the tonal pairs differed, such as C+G and C+F#). Therefore, there were twelve 16-note melodies included per tonality. The chart below outlines the melodies used by breaking down the key, starting note, associated correlation (of the two 8-note melodies and their average, resulting in the correlation for the combined 16-note melody), pitch spread (large pitch spread (LPS) and small pitch spread (SPS)), and interonset interval (IOI small (S, 125ms), IOI medium (M, 250ms) and IOI large (L, 400ms)). The interval differences between each note in the original melodies will be averaged so the melodies can be easily transposed for future purposes (assessing more than the three tonalities outlined in this study). The excel file with the interval differences is included and labelled “melodies (corr + intervals),” with the intervals identified in the sheets labelled “IM C+C,” “IM C+G,” and “IM C+F#”).
Timing (interonset interval, or IOI) also plays a major role in auditory streaming research. Three IOI levels were included, with 12 melodies per IOI level all separated into individual sheets on excel (with the associated midi numbers instead of note names). The IOI’s are identified as IOI_L (large, 400ms), IOI_M (medium, 250ms), and IOI_S (small, 125ms). Each IOI level has its own routine (labelled “trial_IOI_L_400,” “trial_IOI_M_250,” and “trial_IOI_S_125”). There are 12 melodies per routine (4 16-note melodies per tonal pair). The 4 melodies per tonality are outlined in the “project 1 stimuli” chart below (2 melodies per pitch spread (small and large) and alternating starting note). 
The 16-note melodies were composed following the outline of two pitch spreads; large and small. Large pitch spreads require an interval difference of 11 to 13 semitones between each note and small pitch spreads require an interval difference of 3 to 5 semitones between pitches (the only exclusions to these outlines were at the cadences of each melody, following the most common (and expected) cadences according to Western music theory). The melodies were first written in Western musical notation and then converted to midi note number. Midi note numbers were then converted to note letters and associated octaves (for example, midi note number 60 would be C4). Piano recordings of individual pitches were then used in order to compile the melodies (outlined in Excel (files “IOI_L,” “IOI_M,” and “IOI_S”)).
IOI times were varied in each routine in Psychopy (the “start” and “stop” times of sound stimuli), with the outlined spreadsheet (containing the outlined 12 melodies) being called in via the loop (sheets “IOI_L.xlsx,” “IOI_M.xlsx,” and “IOI_S.xlsx”). The presentation of each melody in a routine was randomized, although the presentation of the notes were sequential in order to preserve the pre-determined pitch spread). 
